{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a spark cluster on demand\n",
    "\n",
    "We're going to look at how to create a spark cluster from the notebook using the oshinko REST server and then run the notebook app against it. The cluster configuration that we'll pass in has options set to turn on metrics reporting and the elastic worker daemon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a random cluster name\n",
    "\n",
    "We're going to create an ephemeral cluster, so we aren't too concerned with the cluster name. We just use a name with a random 4 character suffix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "clustername = \"cluster-\" + ''.join(random.SystemRandom().choice(string.ascii_lowercase + string.digits) for _ in range(4))\n",
    "print(\"The cluster will be named \" + clustername)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the IP address of the oshinko-rest server\n",
    "\n",
    "Since the oshinko-rest pod was already running in the project when we launched the notebook, the host and port for the oshinko-rest service can be found in the local environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "oshinko_server = os.environ.get(\"OSHINKO_REST_SERVICE_HOST\") + \":\" + os.environ.get(\"OSHINKO_REST_SERVICE_PORT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending a cluster creation request to oshinko\n",
    "\n",
    "The next step is to send a request to oshinko to create the cluster.\n",
    "\n",
    "### The cluster configuration object\n",
    "\n",
    "Rather than set all the options inline, we'll use a previoulsy created cluster configuration to tell oshinko how to configure the cluster. The configuration is stored in a Kubernetes object called a *configmap* and it's JSON representation looks like this:\n",
    "```json\n",
    "{\n",
    "    \"kind\": \"ConfigMap\",\n",
    "    \"apiVersion\": \"v1\",\n",
    "    \"metadata\": {\n",
    "        \"name\": \"clusterconfig\",\n",
    "        \"creationTimestamp\": null\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"metrics.enable\": \"true\",\n",
    "        \"scorpionstare.enable\": \"true\",\n",
    "        \"sparkimage\": \"docker.io/manyangled/var-spark-worker:latest\",\n",
    "        \"sparkmasterconfig\": \"masterconfig\",\n",
    "        \"sparkworkerconfig\": \"workerconfig\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Notice that we've enabled metrics and the elastic worker daemon (scoripon stare) and specified our custom spark image which contains our pre-loaded data. We also see that the names of two other configmaps, *masterconfig* and *workerconfig*, are used to provide the spark configuration for the master and workers. These other configmaps contain spark configuration files that will be written to $SPARK_HOME/conf in the master and worker containers.\n",
    "\n",
    "One additional detail -- since we did not specify *workercount* here, we will get the default number of workers (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the request and parsing the response\n",
    "\n",
    "To create the cluster, we simply post a request to oshinko giving the cluster name and the configuration.\n",
    "The response from the server will tell us things like how many workers were created and what the URL is for the spark master. In this case, we also want the host and port for the metrics server which by convention will always be the name of the cluster with a suffix of *-metrics* and port 8000 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.post(\"http://%s/clusters\" % oshinko_server,\n",
    "                      json={\"name\": clustername, \n",
    "                            \"config\": {\"name\": \"clusterconfig\"}\n",
    "                           }\n",
    "                 )\n",
    "json = r.json()\n",
    "desired_workers = json['cluster']['config']['workerCount']\n",
    "spark_master = json[\"cluster\"][\"masterUrl\"]\n",
    "metrics_server = clustername + \"-metrics:8000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get fancy -- using the metrics server to monitor worker startup\n",
    "\n",
    "Since we know we've enabled metrics for use by the elastic worker daemon, we can go ahead and use the same metrics server to wait for the workers to register with the master. Here we poll the metrics server until *master.aliveWorkers* is equal to the number of workers that oshinko told us it created. When all of the expected workers have registered with the master we'll drop out of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "def latest_metric_value(metrics_ip, metric):\n",
    "    recent_query = \"http://%s/render\" % metrics_ip\n",
    "    \n",
    "    # We may actually try to contact the metrics service before it's up, so catch connection exceptions here\n",
    "    try:\n",
    "        qr = requests.get(recent_query, params={\"target\": metric, \"format\": \"json\", \"from\": \"-1min\"})\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    if qr.status_code != 200:\n",
    "        sys.stderr.write(\"query code error: %d\\n\" % (qr.status_code))\n",
    "        return None\n",
    "    qj = qr.json()\n",
    "    if len(qj) != 1: return None\n",
    "    dp = [e[0] for e in qj[0][\"datapoints\"] if e[0] is not None]\n",
    "    if len(dp) < 1: return None\n",
    "    return dp[-1]\n",
    "\n",
    "print(\"Waiting for workers\", end=\"\")\n",
    "cnt = 0\n",
    "while True:\n",
    "    workers = latest_metric_value(metrics_server, \"master.aliveWorkers\")\n",
    "    if workers != None and workers >= desired_workers:\n",
    "        break\n",
    "    if cnt % 3 == 0:\n",
    "        print(\"...\",end=\"\")\n",
    "    time.sleep(1)\n",
    "    cnt += 1\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning on metrics reporting for the spark driver (our notebook)\n",
    "\n",
    "In order for the elastic worker daemon to function, the spark driver must also report metrics. We must tell spark what kind of metrics to send, how often, and where to send them. The metrics host by convention will be the cluster name plus a suffix of *-carbon* and it will listen on port 2003. Here we write out the required config to `$SPARK_HOME/conf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "spark_conf = \"%s/conf/\" % os.environ.get(\"SPARK_HOME\")\n",
    "\n",
    "metrics_properties = textwrap.dedent('''\n",
    "    *.sink.graphite.class=org.apache.spark.metrics.sink.GraphiteSink\n",
    "    *.sink.graphite.host=%s\n",
    "    *.sink.graphite.port=2003\n",
    "    *.sink.graphite.period=5\n",
    "    *.sink.graphite.unit=seconds\n",
    "\n",
    "    # Enable JvmSource for instance driver\n",
    "    driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource'''\n",
    ") % (clustername + \"-carbon\")\n",
    "\n",
    "with open(spark_conf + \"metrics.properties\", \"w\") as myfile:\n",
    "    myfile.write(metrics_properties[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning on dynamic allocation in the driver\n",
    "\n",
    "In order for the elastic worker to receive requests for extra executors, dynamic allocation must also be enabled. We set that here and write out the configuration to `$SPARK_HOME/conf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_defaults = textwrap.dedent('''\n",
    "    spark.dynamicAllocation.enabled true\n",
    "    spark.shuffle.service.enabled true\n",
    "    spark.dynamicAllocation.executorIdleTimeout 300s\n",
    "    spark.dynamicAllocation.schedulerBacklogTimeout 1s\n",
    "    spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 5s\n",
    "    spark.dynamicAllocation.maxExecutors 4'''\n",
    ")\n",
    "\n",
    "with open(spark_conf + \"spark-defaults.conf\", \"w\") as myfile:\n",
    "    myfile.write(spark_defaults[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On to the application\n",
    "\n",
    "At this point the spark cluster has been created and both the cluster and the driver are configured for elastic workers. All that remains is to use the *spark_master* value from oshinko to build the spark context and run the application. When the application is done, we'll tell oshinko to delete the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value-at-risk calculations\n",
    "\n",
    "The basic idea behind the value-at-risk calculation is that we're going to look at the historical returns of a portfolio of securities and run many simulations to determine the range of returns we can expect from these.  We can then predict, over a given time horizon, what our expected loss is at a given probability, e.g., we might say that there is less than a 10% chance that the portfolio will lose more than $1,000,000.\n",
    "\n",
    "Note that this is a didactic example and consequently makes some simplifying assumptions about the composition of the portfolio (i.e., only long positions in common stocks, so no options, dividends, or short selling) and the behavior of the market (i.e., day-to-day return percentages are normally-distributed and independent).  Do not use this code to guide actual investment decisions!\n",
    "\n",
    "## Basic setup\n",
    "\n",
    "Here we import the `pyspark` module and set up a `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "spark = SparkSession.builder.master(spark_master).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "We'll start by loading the data (from the WikiEOD dataset of freely-available stock closing prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.load(\"/data/wikieod.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating historical returns\n",
    "\n",
    "We'll use Spark's windowing functions over data frames to determine the percentage change in each security from the previous close to each day's close.  Basically, we'll add a column to our data frame that represents the percentage change from the previous day's close (that is, `lag(\"close\", 1)` when partitioned by ticker symbol and ordered by date) and the current day's close (that is, `col(\"close\")`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, col, avg, variance\n",
    "\n",
    "ddf = df.select(\"ticker\", \"date\", \"close\").withColumn(\"change\", (col(\"close\") / lag(\"close\", 1).over(Window.partitionBy(\"ticker\").orderBy(df[\"date\"])) - 1.0) * 100)\n",
    "ddf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characterizing expected return distributions\n",
    "\n",
    "With the range of changes now available, we can calculate our expected returns for each security.  Since this is a simple example, we'll assume that returns are normal (in the real world, you'd want to use a distribution with heavier tails or a more sophisticated technique altogether).  We can calculate the parameters for each security's distribution as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sqrt\n",
    "mv = ddf.groupBy(\"ticker\").agg(avg(\"change\").alias(\"mean\"), sqrt(variance(\"change\")).alias(\"stddev\"))\n",
    "mv.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are only about 3,000 ticker symbols in our data set, we can easily collect these in driver memory for use in our simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist_map = mv.rdd.map(lambda r: (r[0], (r[1], r[2]))).collectAsMap()\n",
    "dist_map[\"RHT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting current security prices\n",
    "\n",
    "We'll now identify the latest price for each security in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first\n",
    "priceDF = ddf.orderBy(\"date\", ascending=False).groupBy(\"ticker\").agg(first(\"close\").alias(\"price\"), first(\"date\").alias(\"date\"))\n",
    "priceDF.show(10)\n",
    "\n",
    "prices = priceDF.rdd.map(lambda r: (r[0], r[1])).collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a simulation\n",
    "\n",
    "We'll now define our simulation.  This involves three steps: \n",
    "\n",
    "1.  We'll start by generating a random portfolio of securities (a map from ticker symbols to values); then, \n",
    "2.  we'll decide how many simulations to run and generate a random seed for each; and, finally\n",
    "3.  we'll actually run the simulation for a given number of training days, updating the value of our portfolio with random returns sampled from the distribution of historical returns.\n",
    "\n",
    "Generating the random portfolio is pretty straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint, seed\n",
    "\n",
    "def random_portfolio(symbols):\n",
    "    result = {}\n",
    "    for s in symbols:\n",
    "        result[s] = prices[s] * (randint(1, 1000) * 11)\n",
    "    return result\n",
    "\n",
    "def portfolio_value(pf):\n",
    "    return sum([v for v in pf.values()])\n",
    "\n",
    "seed(0xdea110c8)\n",
    "\n",
    "portfolio = random_portfolio(ddf.select(\"ticker\").distinct().sample(True, 0.01, 0xdea110c8).rdd.map(lambda r: r[0]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is generating a collection of random seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seeds(count):\n",
    "    return [randint(0, 1 << 32 - 1) for i in range(count)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a single step of our simulation (and, subsequently, a whole run of the simulation) next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simstep(pf, params, prng):\n",
    "    def daily_return(sym):\n",
    "        mean, stddev = params[sym]\n",
    "        change = (prng.normalvariate(mean, stddev) + 100) / 100.0\n",
    "        return change\n",
    "    return dict([(s, daily_return(s) * v) for s, v in pf.items()])\n",
    "\n",
    "def simulate(seed, pf, params, days):\n",
    "    from random import Random\n",
    "    prng = Random()\n",
    "    prng.seed(seed)\n",
    "    pf = pf.copy()\n",
    "    \n",
    "    for day in range(days):\n",
    "        pf = simstep(pf, params, prng)\n",
    "    return pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to actually run the simulation.  For each seed, we'll spawn a Spark job to simulate 5 days of activity on our portfolio and then return the total dollar value of our gain or loss at the end of the period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "seed_rdd = sc.parallelize(seeds(10000))\n",
    "bparams = sc.broadcast(dist_map)\n",
    "bpf = sc.broadcast(portfolio)\n",
    "initial_value = portfolio_value(portfolio)\n",
    "\n",
    "results = seed_rdd.map(lambda s: portfolio_value(simulate(s, bpf.value, bparams.value, 5)) - initial_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simulated_results = list(zip(results.collect(), seed_rdd.collect()))\n",
    "simulated_values = [v for (v, _) in simulated_results]\n",
    "simulated_values.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = plt.hist(simulated_values, bins=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xvals = [float(i) / len(simulated_values) for i in range(len(simulated_values))]\n",
    "_ = plt.plot(xvals, simulated_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the market historically trends up, we have a better than even chance of not losing money in our simulation.  We can see the 5% value at risk over our time horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simulated_values[int(len(simulated_values) * 0.05)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing random walks\n",
    "\n",
    "Finally, let's look at some example simulation runs to see how the portfolio value changes over time.  We'll take the runs with the best and worst returns and also the runs at each decile.  To visualize our simulation, we'll need a slightly different `simulate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_with_history(seed, pf, params, days):\n",
    "    from random import Random\n",
    "    prng = Random()\n",
    "    prng.seed(seed)\n",
    "    pf = pf.copy()\n",
    "    values = [portfolio_value(pf)]\n",
    "    \n",
    "    for day in range(days):\n",
    "        pf = simstep(pf, params, prng)\n",
    "        values.append(portfolio_value(pf))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now repeat the simulation on eleven of our seeds from the earlier run, collecting history for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simulated_results.sort()\n",
    "eleven_results = [simulated_results[int((len(simulated_results) - 1) * i / 10)] for i in range(11)]\n",
    "eleven_seeds = sc.parallelize([seed for (_, seed) in eleven_results])\n",
    "walks = eleven_seeds.map(lambda s: simulate_with_history(s, bpf.value, bparams.value, 5))\n",
    "\n",
    "walk_results = walks.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = plt.plot([list(c) for c in zip(*walk_results)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting our ephemeral cluster\n",
    "\n",
    "If we do not want to keep the cluster around for other applications, we can use oshinko to delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.delete(\"http://%s/clusters/%s\" % (oshinko_server, clustername))\n",
    "if r.status_code == 204:\n",
    "    print(\"Cluster %s has been deleted\" % clustername)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
